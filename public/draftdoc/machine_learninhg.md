机器学习 Machine Learning - by 2021.02
========================
机器学习这个词在几年前确实有点超前，人们往往无法把它和生活所结合起来，但在当今社会如果你是一个细心观察的人，你会发现机器学习已经无处不在。
例如：
- google的新闻聚类，google每天收集大量的新闻，对具有相同特征的新闻进行分簇；
- 游戏中对非常坑的队友人身攻击，会被文字模型预测为人身攻击，从而禁止你输出；
- 直播平台的弹幕自动被视频当前人像隐藏，利用人像关键点识别，即可抓取人像出现的坐标点；
- 等等～

机器学习是人工智能的一个发展方向，机器学习和深度学习都属于人工智能的一个子集。

机器学习又分为：
- 监督学习
- 无监督学习
- 强化学习

## What is Machine Learning?
Arthur Samuel (1959): 在没有明确设定的情况下，使计算机具有学习能力的研究领域。

Tom Mitchell (1998): 计算机程序从经验E中学习，解决某一任务T进行某一性能度量P，通过P测定在T上的表现因经验E而提高。

李宏毅：机器学习就是让机器自己找函式。
f(音频) => '文字'
f(图片) => '图片分类'
往往这样的函式，我们无法实现，这时候就需要机器通过学习，给我们函式。

## 监督学习 Supervised learning
根据已有的数据集，知道输入和输出结果之间的关系。根据这种已知的关系，训练得到一个最优的模型。
也就是说，在监督学习中训练数据既有特征(feature)又有标签(label)，通过训练，让机器可以自己找到特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断出标签。

监督学习的分类：
* 回归：Regression
对已存在的训练数据进行分析，拟合出适当的函数模型y=f(x)，这里的y就是标签，而对一个新的数据特征x，通过这个函数模型就能得到标签y。

* 分类：Classification
预测离散值输出，输出是有限的。
肿瘤分类问题良性还是恶性，根据其肿瘤特征size、age，来预测新肿瘤的类型。

* 问题
人脸识别是分类还是回归问题，人脸识别是一个分类问题


## 无监督学习 Unsuoervised learning
缺乏足够的先验知识，因此难以人工标注类别或进行人工类别标注的成本太高，我们希望计算机来完成这些工作。根据类别未知(没有被标记)的训练样来进行识别，称之为无监督学习。

无监督学习里典型例子是聚类，把相似的东西聚在一起，我们不关心这一类是什么，因此聚类算法通常只需要知道如何计算相似度就可以开始工作了。

* 给定一个数据集没有标签信息，通过聚类算法把数据进行分簇。
* Google的新闻分类，对大量的新闻进行分簇，把具有相似特征的新闻分到一起。
* 根据给定基因数据，对DNA数据进行分簇。


## 线性回归模型 Linear Regression
给定一组训练数据，根据特征和标签之间的关系来拟合一条直线或曲线，该直线就是一个线性回归模型，通过该模型可以预测新特征所对应的标签。
用一元二次方程表示 -> y = w*x + b -> H(x) = θ[0] + θ[1] * x
自变量y随因变量x的改变而改变，通过大量的数据{x, y} 求导w和b的值，让我们输入一个新的x时能预测新的y。怎么才能让表达式更加准确呢。我们就需要对模型进行损失比较，当损失达到最小的时候，也就是该模型预测最准确的时候。

* 符号
 - m 训练样本数量
 - x 输入变量，特征
 - y 输出变量，预测的目标变量
 - (x, y) 表示一个训练样本

* 损失函数
拟合得到的直线或曲线，如何去评判它的标准呢，哪条直线才是最完美的呢，也就是性能度量。
LOSS = 1/2m * ∑i=1(f(x[i]) - y^[i])**2


## 贝叶斯公式
一座别墅 20年 被盗2次      P(B) = 2 / 20 * 365
主人的狗 每周 叫3次        P(A) = 3 / 7
盗贼入侵时狗叫的概率为0.9   P(A|B) = 0.9
狗叫的时候盗贼入侵的概率    P(B|A) = ?
盗贼入侵狗叫同时发生的概率   P(AB) / P(A∩B)

P(A|B) = P(A∩B) / P(B)
P(B|A) = P(A|B) * P(B) / P(A)

### 条件概率
当一个事件发生另一个事件发生的概率 -> 当盗贼入侵(B)时狗叫(A)的概率 -> P(A|B)
### 先验概率
先验概率不是根据有关自然状态的全部资料测定的，而只是利用现有的材料(主要是历史资料)计算的；
后验概率使用了有关自然状态更加全面的资料，既有先验概率资料，也有补充资料；
上面的别墅被盗P(A)、狗叫P(B)都是先验概率，他们都是基于历史数据的测定；
### 联合概率
表示两个事件同时发生的概率 -> P(A∩B) 或者 P(AB)


## 奇异值分解
就是把一个矩阵分解为三个矩阵相乘
M = UΣVT
M是一个m*n阶的矩阵
U是m阶矩阵 m * m  酉矩阵 满足UT * U = I
V是n阶矩阵 n * n  酉矩阵 满足VT * V = I
Σ对角矩阵  m * n  只有主对角线有值，其余全为0